<!DOCTYPE html>
<html class="writer-html5" lang="ja" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>3-1【デモ】API での推論（AzureAIを使用した画像とテキスト分析） &mdash; Sphinx  ドキュメント</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../_static/nbsphinx-code-cells.css?v=2aa19091" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=c033477b"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="../_static/translations.js?v=4dbe4bdc"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="検索" href="../search.html" />
    <link rel="next" title="4.【事例共有】事例から学ぶ AI の導入" href="04.html" />
    <link rel="prev" title="2.機械学習 プロジェクト全体像" href="02.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Sphinx
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="0.html">〜研修へようこそ</a></li>
<li class="toctree-l1"><a class="reference internal" href="0.html#0.AI-ビジネス-アジェンダ企画">0.AI ビジネス アジェンダ企画</a></li>
<li class="toctree-l1"><a class="reference internal" href="01.html">1.AI とは、機械学習・DLとのすみわけ</a></li>
<li class="toctree-l1"><a class="reference internal" href="02-1.html">1-1【演習】画像分類モデルの作成</a></li>
<li class="toctree-l1"><a class="reference internal" href="02.html">2.機械学習 プロジェクト全体像</a></li>
<li class="toctree-l1"><a class="reference internal" href="02.html#3.AI-プロジェクトの開発プロセス">3.AI プロジェクトの開発プロセス</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">3-1【デモ】API での推論（AzureAIを使用した画像とテキスト分析）</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#コートの実装">コートの実装</a></li>
<li class="toctree-l2"><a class="reference internal" href="#環境構築">環境構築</a></li>
<li class="toctree-l2"><a class="reference internal" href="#画像分析">画像分析</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#分析を行う画像の読み込み">分析を行う画像の読み込み</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#ランドマーク（建物の検出）">ランドマーク（建物の検出）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#文章生成">文章生成</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#まとめ">まとめ</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="04.html">4.【事例共有】事例から学ぶ AI の導入</a></li>
<li class="toctree-l1"><a class="reference internal" href="05.html">【ワーク】AI プロジェクト立案</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Sphinx</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">3-1【デモ】API での推論（AzureAIを使用した画像とテキスト分析）</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/src/03.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="3-1【デモ】API-での推論（AzureAIを使用した画像とテキスト分析）">
<h1>3-1【デモ】API での推論（AzureAIを使用した画像とテキスト分析）<a class="headerlink" href="#3-1【デモ】API-での推論（AzureAIを使用した画像とテキスト分析）" title="Link to this heading"></a></h1>
<p>Lab 1 にて作成した、Azure AIリソースを使用して、機械学習モデルを使用した推論を行います。 今回は画像処理を行います。</p>
<div class="line-block">
<div class="line">実装環境として Azure Machine Learning というサービスを利用します。</div>
<div class="line">Azure のトップページからリソースの作成をしてください。</div>
</div>
<p>コンピューティングから新規を選択します。 - コンピューティング名: 名字+日付+名前の頭文字（koike0401a） - 仮想マシンのサイズ: Standard_D2d_v5 - その他、デフォルト設定のまま作成を選択。</p>
<p>Notebook を開く。 下記の＋からファイルのアップロードを選択、<a class="reference internal" href="img/Untitled.html"><span class="doc">こちら</span></a>のNotebookをアップロードする。</p>
<div class="line-block">
<div class="line">【ノートブックの設定】 - コンピューティング：先ほど作成したコンピューティングを選択する。</div>
<div class="line">- カーネル: Python 3.~ AzureML</div>
</div>
<p>ノートブックをアップロードし、リージョンとキーの置き換えができたら順に画像処理を実装していきましょう。　Shift + ENTER でコードを実行できます。</p>
<section id="コートの実装">
<h2>コートの実装<a class="headerlink" href="#コートの実装" title="Link to this heading"></a></h2>
<p>以下のコードがアップロードされたノートブックに含まれています。 コード自体を暗記する必要はないですがどのような手順でどのような処理を実装しているのか抑えましょう。</p>
</section>
<section id="環境構築">
<h2>環境構築<a class="headerlink" href="#環境構築" title="Link to this heading"></a></h2>
<p>実装をするための環境の読み込みを行います。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>!pip install azure-cognitiveservices-vision-computervision
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Requirement already satisfied: azure-cognitiveservices-vision-computervision in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (0.9.0)
Requirement already satisfied: azure-common~=1.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-cognitiveservices-vision-computervision) (1.1.28)
Requirement already satisfied: msrest&gt;=0.5.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-cognitiveservices-vision-computervision) (0.7.1)
Requirement already satisfied: requests~=2.16 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from msrest&gt;=0.5.0-&gt;azure-cognitiveservices-vision-computervision) (2.31.0)
Requirement already satisfied: isodate&gt;=0.6.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from msrest&gt;=0.5.0-&gt;azure-cognitiveservices-vision-computervision) (0.6.1)
Requirement already satisfied: azure-core&gt;=1.24.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from msrest&gt;=0.5.0-&gt;azure-cognitiveservices-vision-computervision) (1.26.4)
Requirement already satisfied: requests-oauthlib&gt;=0.5.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from msrest&gt;=0.5.0-&gt;azure-cognitiveservices-vision-computervision) (1.3.1)
Requirement already satisfied: certifi&gt;=2017.4.17 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from msrest&gt;=0.5.0-&gt;azure-cognitiveservices-vision-computervision) (2022.9.24)
Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests~=2.16-&gt;msrest&gt;=0.5.0-&gt;azure-cognitiveservices-vision-computervision) (3.1.0)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests~=2.16-&gt;msrest&gt;=0.5.0-&gt;azure-cognitiveservices-vision-computervision) (1.26.16)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests~=2.16-&gt;msrest&gt;=0.5.0-&gt;azure-cognitiveservices-vision-computervision) (3.4)
Requirement already satisfied: six in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from isodate&gt;=0.6.0-&gt;msrest&gt;=0.5.0-&gt;azure-cognitiveservices-vision-computervision) (1.16.0)
Requirement already satisfied: typing-extensions&gt;=4.3.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-core&gt;=1.24.0-&gt;msrest&gt;=0.5.0-&gt;azure-cognitiveservices-vision-computervision) (4.6.0)
Requirement already satisfied: oauthlib&gt;=3.0.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests-oauthlib&gt;=0.5.0-&gt;msrest&gt;=0.5.0-&gt;azure-cognitiveservices-vision-computervision) (3.2.2)
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>#　モジュールの読み込み
from azure.cognitiveservices.vision.computervision import ComputerVisionClient
from azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes
from msrest.authentication import CognitiveServicesCredentials
from PIL import Image
import os
# 下記でAzure AI サービスを利用するための認証情報を入力します。自身のAzure AIリソースからコピペを行ってください。*実務ではenvファイルなど別ファイルに認証情報は格納します。
key = &#39;Azure AI サービスの key&#39;
credentials = CognitiveServicesCredentials(key)
client = ComputerVisionClient(
    endpoint=&quot;Azure AI サービスのエンドポイント&quot;,
    credentials=credentials
)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import requests
import io
from io import BytesIO
import urllib
from urllib.request import urlopen
</pre></div>
</div>
</div>
</section>
<section id="画像分析">
<h2>画像分析<a class="headerlink" href="#画像分析" title="Link to this heading"></a></h2>
<p>まずは画像分析から行っていきます。 ComputerVisionClient というメソッドを使用するにことによって、次のことができます。</p>
<ul class="simple">
<li><p>画像の分析: 顔、色、タグなど、特定の機能の画像を分析できます。</p></li>
<li><p>画像の説明を取得する: そのサブジェクト ドメインに基づいて画像の説明を取得します。</p></li>
</ul>
<p>では実際にコードを実行してみましょう。</p>
<section id="分析を行う画像の読み込み">
<h3>分析を行う画像の読み込み<a class="headerlink" href="#分析を行う画像の読み込み" title="Link to this heading"></a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><br/><span></span>import requests
import numpy as np
from PIL import Image
from io import BytesIO

# 画像のURL
image_url = &quot;https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Broadway_and_Times_Square_by_night.jpg/450px-Broadway_and_Times_Square_by_night.jpg&quot;

# 画像を取得
response = requests.get(image_url)
image_bytes = BytesIO(response.content)

# PILを使用して画像を開く
image_pil = Image.open(image_bytes)
display(image_pil)
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/src_03_10_0.png" src="../_images/src_03_10_0.png" />
</div>
</div>
<p>上記の画像を使用し、分析を行います。まずは画像に写っている情報の検出を行いましょう。 ### 画像分析の実装</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># image_analytics で上の画像分析する
image_analysis = client.analyze_image(image_url,visual_features=[VisualFeatureTypes.tags])

for tag in image_analysis.tags:
    print(tag)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
{&#39;additional_properties&#39;: {}, &#39;name&#39;: &#39;building&#39;, &#39;confidence&#39;: 0.9910045862197876, &#39;hint&#39;: None}
{&#39;additional_properties&#39;: {}, &#39;name&#39;: &#39;metropolis&#39;, &#39;confidence&#39;: 0.9403555393218994, &#39;hint&#39;: None}
{&#39;additional_properties&#39;: {}, &#39;name&#39;: &#39;metropolitan area&#39;, &#39;confidence&#39;: 0.9358731508255005, &#39;hint&#39;: None}
{&#39;additional_properties&#39;: {}, &#39;name&#39;: &#39;downtown&#39;, &#39;confidence&#39;: 0.9340376853942871, &#39;hint&#39;: None}
{&#39;additional_properties&#39;: {}, &#39;name&#39;: &#39;outdoor&#39;, &#39;confidence&#39;: 0.9233906269073486, &#39;hint&#39;: None}
{&#39;additional_properties&#39;: {}, &#39;name&#39;: &#39;skyscraper&#39;, &#39;confidence&#39;: 0.9208872318267822, &#39;hint&#39;: None}
{&#39;additional_properties&#39;: {}, &#39;name&#39;: &#39;urban area&#39;, &#39;confidence&#39;: 0.9175583124160767, &#39;hint&#39;: None}
{&#39;additional_properties&#39;: {}, &#39;name&#39;: &#39;street&#39;, &#39;confidence&#39;: 0.8893557786941528, &#39;hint&#39;: None}
{&#39;additional_properties&#39;: {}, &#39;name&#39;: &#39;commercial building&#39;, &#39;confidence&#39;: 0.8842802047729492, &#39;hint&#39;: None}
{&#39;additional_properties&#39;: {}, &#39;name&#39;: &#39;mixed-use&#39;, &#39;confidence&#39;: 0.8771032094955444, &#39;hint&#39;: None}
{&#39;additional_properties&#39;: {}, &#39;name&#39;: &#39;crowded&#39;, &#39;confidence&#39;: 0.8658456802368164, &#39;hint&#39;: None}
{&#39;additional_properties&#39;: {}, &#39;name&#39;: &#39;night&#39;, &#39;confidence&#39;: 0.8426163196563721, &#39;hint&#39;: None}
{&#39;additional_properties&#39;: {}, &#39;name&#39;: &#39;city&#39;, &#39;confidence&#39;: 0.8208400011062622, &#39;hint&#39;: None}
{&#39;additional_properties&#39;: {}, &#39;name&#39;: &#39;people&#39;, &#39;confidence&#39;: 0.6946084499359131, &#39;hint&#39;: None}
{&#39;additional_properties&#39;: {}, &#39;name&#39;: &#39;light&#39;, &#39;confidence&#39;: 0.6930656433105469, &#39;hint&#39;: None}
</pre></div></div>
</div>
<p>建物、メトロポリス、ダウンタウン、外、街、人、ライトなど画像から様々な情報が取得できていることがわかります。 confidence はモデルの自信度を表しており、どのくらい予測結果に自身があるのかどうかを表しています。</p>
<p>確認をしてみると人（People）などは画像もぼやけており、自信度も低くなっていることがわかります。</p>
<div class="line-block">
<div class="line"><strong>可能な方は自身の好きな画像を使用し、推論を行ってみてください。image_url の URL を変更してみましょう</strong></div>
<div class="line">ほんの数コードで高度な画像処理を行うことができました。</div>
</div>
</section>
</section>
<section id="ランドマーク（建物の検出）">
<h2>ランドマーク（建物の検出）<a class="headerlink" href="#ランドマーク（建物の検出）" title="Link to this heading"></a></h2>
<p>次にlist_models という種類のAIモデル表示します。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>models = client.list_models()

for x in models.models_property:
    print(x)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
{&#39;additional_properties&#39;: {}, &#39;name&#39;: &#39;landmarks&#39;, &#39;categories&#39;: [&#39;outdoor_&#39;, &#39;户外_&#39;, &#39;屋外_&#39;, &#39;aoarlivre_&#39;, &#39;alairelibre_&#39;, &#39;building_&#39;, &#39;建筑_&#39;, &#39;建物_&#39;, &#39;edifício_&#39;]}
</pre></div></div>
</div>
<div class="line-block">
<div class="line">こちらは画像から建物などを検出してくれるモデルとなっているようです。</div>
<div class="line">実際に使用してみましょう。</div>
<div class="line">他にも様々なモデルがあり、OCRや物体検出などが行えます。使用できるモデルは<a class="reference external" href="https://learn.microsoft.com/ja-jp/python/api/azure-cognitiveservices-vision-computervision/azure.cognitiveservices.vision.computervision.operations.computervisionclientoperationsmixin?view=azure-python-preview">こちら</a>をご確認ください。 では、また画像を読み込み、推論を行ってみましょう。</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>domain = &quot;landmarks&quot;
url = &quot;https://images.pexels.com/photos/338515/pexels-photo-338515.jpeg&quot;

# 画像を取得
response = requests.get(url)
image_bytes = BytesIO(response.content)

# PILを使用して画像を開く
image_pil = Image.open(image_bytes)
display(image_pil)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/src_03_18_0.png" src="../_images/src_03_18_0.png" />
</div>
</div>
<p>では、上記の画像を list_model に推論させてみましょう。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>#画像のLandmarks の取得
language = &quot;ja&quot;

analysis = client.analyze_image_by_domain(domain, url, language)

for landmark in analysis.result[&quot;landmarks&quot;]:
    print(landmark[&quot;name&quot;])
    print(landmark[&quot;confidence&quot;])
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
エッフェル塔
0.971265435218811
</pre></div></div>
</div>
<div class="line-block">
<div class="line">画像にエッフェル塔が写っていることを検出してくれています。</div>
<div class="line">また、コードにて言語（language)を日本語に設定しているため推論結果も日本語で検出されています。この用に英語だけではない多言語にも対応していることがわかりました。他にも有名人などをこのモデルでは検出することが可能です。</div>
</div>
<section id="文章生成">
<h3>文章生成<a class="headerlink" href="#文章生成" title="Link to this heading"></a></h3>
<div class="line-block">
<div class="line">最後に先程のエッフェル塔が検出された画像から画像の説明文の作成を行います。</div>
<div class="line">describe_image のメソッドを使用すると簡単に画像から説明文を作成してくれます。実際に実装してみましょう。</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 画像の説明文の作成
domain = &quot;landmarks&quot;
language = &quot;ja&quot;
max_descriptions = 3

analysis = client.describe_image(url, max_descriptions, language)

for caption in analysis.captions:
    print(caption.text)
    print(caption.confidence)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
建物の前に立っているエッフェル塔
0.3260513366368707
時計台のあるエッフェル塔
0.3250513366368707
草の上に立っているエッフェル塔
0.3240513366368707
</pre></div></div>
</div>
<p>画像から説明文が生成されました。AI のConfidence（自信度）があまり高くないため少し違和感のある文章ではありますが、画像がどのような画像なのかは伝わるかと思います。実装は以上です。クラウドを使用して高度な画像処理を実装しました。最後に下記について考えてみましょう。</p>
</section>
</section>
<section id="まとめ">
<h2>まとめ<a class="headerlink" href="#まとめ" title="Link to this heading"></a></h2>
<p>実装が終わったら下記について考えて見ましょう。 - Azure AI サービスでの画像処理とローカルでの画像処理モデルの違いはなんですか？ - Azure AI サービスのメリットはなんですか？ - Azure AI サービスのデメリット・考慮事項は何でしょうか？ - ローカルで画像処理モデルを構築し、活用する場合と Azure AI サービスはどの用に使い分けますか？ - ChatGPT など生成 AI との違いはなんですか？ - 実装した AI はどのようなユースケースで使用できそうですか？</p>
<div class="line-block">
<div class="line">クラウドの推論モデルを使用し簡単に実装することができました。</div>
<div class="line">ビジネス目線でAPIモデルのメリットとデメリットを洗い出しましょう。</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="02.html" class="btn btn-neutral float-left" title="2.機械学習 プロジェクト全体像" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="04.html" class="btn btn-neutral float-right" title="4.【事例共有】事例から学ぶ AI の導入" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, trainocate.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>